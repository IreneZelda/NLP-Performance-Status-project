{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d75f80",
   "metadata": {},
   "source": [
    "### 7.Evaluation binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#load validation results\n",
    "val_bin_results = pd.read_csv(r'.\\Results\\acsess_bin_results_val.csv', sep = '\\t')\n",
    "#load test results\n",
    "test_bin_results = pd.read_csv(r'.\\Results\\acsess_bin_results_test.csv', sep = '\\t')\n",
    " \n",
    "\n",
    "### Change according to number of examples used during training\n",
    "k=0  #zero-shot\n",
    "#k=1 #one-shot\n",
    "#k=5 #few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bin_results.columns = val_bin_results.columns.str.replace('binary ', '', regex=False)\n",
    "test_bin_results.columns = test_bin_results.columns.str.replace('binary ', '', regex=False)\n",
    "#grouped = bin_results.groupby('note_nr').agg({'k=0': 'max', 'k=1': 'max', 'k=5': 'max'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {f'k={i}': {'precision': [], 'recall': [], 'f1_score': []} for i in range(k)} \n",
    "\n",
    "    # Loop over each fold and each k\n",
    "    for fold in ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']:\n",
    "        for k in range(k): \n",
    "            # Filter the data for the current fold\n",
    "            fold_data = df[df['fold'] == fold]\n",
    "\n",
    "            # Get the ground truth and predictions, ignoring NaN values\n",
    "            y_true = fold_data['relevance_manual']\n",
    "            y_pred = fold_data[f'k={k}'].dropna()\n",
    "            y_true = y_true[y_pred.index]\n",
    "\n",
    "            # Calculate precision, recall, and F1-score\n",
    "            print(y_true)\n",
    "            print(y_pred)\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "            # Append the scores to the results dictionary\n",
    "            results[f'k={k}']['precision'].append(precision)\n",
    "            results[f'k={k}']['recall'].append(recall)\n",
    "            results[f'k={k}']['f1_score'].append(f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_results_table(df):\n",
    "    # Calculate metrics\n",
    "    results = calculate_metrics(df)\n",
    "\n",
    "    # Calculate averages and standard deviations for each k\n",
    "    precision_avg_list, precision_std_list = [], []\n",
    "    recall_avg_list, recall_std_list = [], []\n",
    "    f1_avg_list, f1_std_list = [], []\n",
    "    \n",
    "    for k in range(k): \n",
    "        print(results[f'k={k}']['precision'])\n",
    "        precision_avg = np.mean(results[f'k={k}']['precision'])\n",
    "        precision_std = np.std(results[f'k={k}']['precision'])\n",
    "        recall_avg = np.mean(results[f'k={k}']['recall'])\n",
    "        recall_std = np.std(results[f'k={k}']['recall'])\n",
    "        f1_avg = np.mean(results[f'k={k}']['f1_score'])\n",
    "        f1_std = np.std(results[f'k={k}']['f1_score'])\n",
    "\n",
    "        precision_avg_list.append(precision_avg)\n",
    "        precision_std_list.append(precision_std)\n",
    "        recall_avg_list.append(recall_avg)\n",
    "        recall_std_list.append(recall_std)\n",
    "        f1_avg_list.append(f1_avg)\n",
    "        f1_std_list.append(f1_std)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_table = pd.DataFrame({\n",
    "        'k': [f'k={i}' for i in range(k)], \n",
    "        'Precision (Avg)': precision_avg_list,\n",
    "        'Precision (Std)': precision_std_list,\n",
    "        'Recall (Avg)': recall_avg_list,\n",
    "        'Recall (Std)': recall_std_list,\n",
    "        'F1-score (Avg)': f1_avg_list,\n",
    "        'F1-score (Std)': f1_std_list\n",
    "    })\n",
    "\n",
    "    return results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results validation\n",
    "print(\"ACSESS validation results\")\n",
    "results_table_val = generate_results_table(val_bin_results)\n",
    "print(results_table_val.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results test\n",
    "print(\"ACSESS test results\")\n",
    "results_table_test = generate_results_table(test_bin_results)\n",
    "print(results_table_test.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyDRE",
   "language": "python",
   "name": "mydre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

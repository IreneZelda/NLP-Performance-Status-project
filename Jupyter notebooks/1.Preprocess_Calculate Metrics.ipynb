{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6fc428",
   "metadata": {},
   "source": [
    "# 1.Preprocess validation and test set_Calculate Metrics\n",
    "This notebook reads in the csv/excel files with the unprocessed text, split it into sentences and maps it to the labels coming from INCEpTION. In addition to this, calculates some metrics to evaluate the performance of the annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import copy\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "extra_abbreviations = ['Dr', 'Mvw', 'Conclusie', 'SpeciÃ«le anamnese','Familieanamnese','Beleid']\n",
    "tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e09c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_nr='first' #To be repeated for each round of annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read data coming from INCEpTION (https://inception-project.github.io), coming from Annotator 1 and Annotator 2\n",
    "## Must be performed for all rounds of annotations, for this project we had 3 rounds.\n",
    "#Sample_selec contains all notes selected for annotations, including the note number, patient pseudo ID and the report type (e.g. multi-disciplinary meeting, pre-operative, family consult etc.)\n",
    "\n",
    "#annotator1=pd.read_csv('')\n",
    "#annotator1=pd.read_csv('')\n",
    "#ann1['annotator'] = 'annotator1'\n",
    "#ann2['annotator'] = 'annotator2'\n",
    "#annotations = pd.concat([ann1, ann2], ignore_index=True)\n",
    "\n",
    "#sample_selec=pd.read.csv('')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02106615",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function was useful in our data, where the note number was written in the text. Can be ignored\n",
    "\n",
    "def extract_note_nr(text):\n",
    "    matches = re.findall(r'\\d+', text)\n",
    "    return matches[-1] if matches else None\n",
    "\n",
    "annotations['note_nr'] = annotations['source_file'].apply(extract_note_nr)\n",
    "annotations['note_nr'] = annotations['note_nr'].astype(int)\n",
    "\n",
    "\n",
    "# fix any NaN-mistakes\n",
    "annotations = annotations.dropna(subset=['note_nr'])\n",
    "sample_selec = sample_selec.dropna(subset=['note_nr'])\n",
    "\n",
    "# set all note_nrs to same type for consistency\n",
    "annotations['note_nr'] = annotations['note_nr'].astype(int)\n",
    "sample_selec['note_nr'] = sample_selec['note_nr'].astype(int)\n",
    "sample_selec1['note_nr'] = sample_selec1['note_nr'].astype(int)\n",
    "sample_selec2['note_nr'] = sample_selec2['note_nr'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a47c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new '_sentence_text' column and fill with snippets (snippets selected by annotators)\n",
    "\n",
    "#INCEpTION has a different sentence splitting method. This function helps to assign to each sentence the right label by getting a snippet of text around each sentence.\n",
    "def get_snippet(row):\n",
    "    note_nr = row['note_nr']\n",
    "    begin = int(row['begin']) - 73  #this offset must be set specifically for your dataset\n",
    "    end = int(row['end']) -30 #this offset must be set specifically for your dataset\n",
    "    #print(row)\n",
    "    # Look up the note number in sample_selec\n",
    "    censored_text = sample_selec[sample_selec['note_nr'] == note_nr]['censored'].values[0]\n",
    "    \n",
    "    # Get the snippet from the censored text\n",
    "    snippet = censored_text[begin:end]\n",
    "    \n",
    "    return snippet\n",
    "\n",
    "# Rename '_sentence_text' to 'inception_sentence'\n",
    "annotations.rename(columns={'_sentence_text': 'inception_sentence'}, inplace=True)\n",
    "annotations['_sentence_text'] = annotations.apply(get_snippet, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7787f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match snippets from annotators with original sentences from dataset\n",
    "sample_selec['sentences'] = sample_selec['censored'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinguish annotators\n",
    "annotator1 = annotations[annotations['annotator'] == 'annotator1']\n",
    "annotator2 = annotations[annotations['annotator'] == 'annotator2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match snippets annotators to original dataset\n",
    "def find_match(annotator1, note_nr, sentence, sentence_context, sentences, i, threshold=90):\n",
    "    def is_fuzzy_match(text1, text2, threshold=90):\n",
    "        return fuzz.partial_ratio(text1, text2) >= threshold\n",
    "    \n",
    "    match = annotator1[(annotator1['note_nr'] == note_nr) & \n",
    "                       (annotator1['_sentence_text'].apply(lambda x: is_fuzzy_match(x, sentence)) |\n",
    "                        annotator1['_sentence_text'].apply(lambda x: is_fuzzy_match(x, sentence_context)) |\n",
    "                        annotator1['_sentence_text'].apply(lambda x: is_fuzzy_match(sentence, x)) |\n",
    "                        annotator1['_sentence_text'].apply(lambda x: (\n",
    "                            is_fuzzy_match(x[:10], sentence) and (i-1 >= 0 and is_fuzzy_match(x[10:], sentences[i-1]))\n",
    "                        ) or (\n",
    "                            is_fuzzy_match(x[-10:], sentence) and (i+1 < len(sentences) and is_fuzzy_match(x[:-10], sentences[i+1]))\n",
    "                        )))]\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ba969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels for annotator1\n",
    "\n",
    "\n",
    "context_flag=1\n",
    "\n",
    "# Assuming sample_selec and annotator1 are already defined DataFrames\n",
    "\n",
    "# Copy sample_selec to annotator1_results\n",
    "annotator1_results = copy.deepcopy(sample_selec)\n",
    "annotator1_note_nrs = list(sample_selec1['note_nr'])\n",
    "annotator1_results = annotator1_results[annotator1_results['note_nr'].isin(annotator1_note_nrs)]\n",
    "\n",
    "\n",
    "# Create a new column 'manual_sentence_labels' by copying the 'sentences' column\n",
    "annotator1_results['manual_sentence_labels'] = annotator1_results['sentences'].copy()\n",
    "annotator1_results['context_sentences']=annotator1_results['sentences'].copy\n",
    "\n",
    "# Iterate over the rows in annotator1_results\n",
    "for idx, row in annotator1_results.iterrows():\n",
    "    print(idx)\n",
    "    note_nr = row['note_nr']\n",
    "    for i in range(len(row.sentences)):\n",
    "        sentences2=[]\n",
    "        if context_flag:\n",
    "        # Check if sentences[i-1] exists\n",
    "            if i - 1 >= 0:\n",
    "                context.append(sentences[i-1])\n",
    "        # Add sentences[i] (it is assumed that sentences[i] always exists)\n",
    "        context.append(sentences[i])\n",
    "\n",
    "        if context_flag:\n",
    "        # Check if sentences[i+1] exists\n",
    "            if i + 1 < len(sentences):\n",
    "                context.append(sentences[i+1])\n",
    "        # Join the valid sentences with spaces\n",
    "        sentence_context = '.'.join(context)\n",
    "        sentences2.append(sentence_context)\n",
    "    annotator1_results.loc[idx,'context_sentences']=sentences2\n",
    "    \n",
    "    if note_nr in annotator1['note_nr'].values:\n",
    "        for i in range(len(row.sentences)):\n",
    "            sentences2=[]\n",
    "            if context_flag:\n",
    "            # Check if sentences[i-1] exists\n",
    "                if i - 1 >= 0:\n",
    "                    context.append(sentences[i-1])\n",
    "            # Add sentences[i] (it is assumed that sentences[i] always exists)\n",
    "            context.append(sentences[i])\n",
    "\n",
    "            if context_flag:\n",
    "            # Check if sentences[i+1] exists\n",
    "                if i + 1 < len(sentences):\n",
    "                    context.append(sentences[i+1])\n",
    "            # Join the valid sentences with spaces\n",
    "            sentence_context = '.'.join(context)\n",
    "            sentences2.append(sentence_context)\n",
    "        annotator1_results.loc[idx,'context_sentences']=sentences2\n",
    "        # Get the corresponding sentences for the note_nr\n",
    "        sentences = row['context_sentences']#row['manual_sentence_labels']\n",
    "        updated_sentences = []\n",
    "#         for sentence in sentences:\n",
    "#             print('sentence:', sentence)\n",
    "#             match = annotator1[(annotator1['note_nr'] == note_nr) & (annotator1['_sentence_text'] == sentence)]\n",
    "#             if not match.empty:\n",
    "#                 updated_sentences.append(match['WHOPS'].values[0])\n",
    "#             else:\n",
    "#                 updated_sentences.append(np.nan)\n",
    "\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            # Initialize an empty list to hold the valid sentences\n",
    "            context = []\n",
    "            #if context_flag:\n",
    "            # Check if sentences[i-1] exists\n",
    "            #    if i - 1 >= 0:\n",
    "            #        context.append(sentences[i-1])\n",
    "            # Add sentences[i] (it is assumed that sentences[i] always exists)\n",
    "            context.append(sentences[i])\n",
    "            \n",
    "            #if context_flag:\n",
    "            # Check if sentences[i+1] exists\n",
    "            #    if i + 1 < len(sentences):\n",
    "            #        context.append(sentences[i+1])\n",
    "            # Join the valid sentences with spaces\n",
    "            sentence_context = '.'.join(context)\n",
    "            #print('sentence:', sentence)\n",
    "            #print('sentence_context', sentence_context)\n",
    "            # Find the match using the function\n",
    "            result = find_match(annotator1, note_nr, sentence, sentence_context, sentences, i)\n",
    "            #sentences2.append(sentence_context)\n",
    "            # Append the corresponding value or np.nan to updated_sentences\n",
    "            if not result.empty:\n",
    "                if result['WHOPS'].values[0] == 'None':\n",
    "                    updated_sentences.append(np.nan)\n",
    "                else:\n",
    "                    updated_sentences.append(result['WHOPS'].values[0])\n",
    "            else:\n",
    "                updated_sentences.append(np.nan)\n",
    "        # Update the 'manual_sentence_labels' column with the new values\n",
    "        #annotator1_results.at[idx,'sentences']=sentences2\n",
    "        annotator1_results.at[idx, 'manual_sentence_labels'] = updated_sentences\n",
    "        \n",
    "# Iterate over the rows in annotator1_results again to update non-annotated sentences\n",
    "for idx, row in annotator1_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    sentences = row['manual_sentence_labels']\n",
    "    if all(isinstance(s, str) for s in sentences):\n",
    "        match = annotator1[annotator1['note_nr'] == note_nr]\n",
    "        if match.empty:\n",
    "            annotator1_results.at[idx, 'manual_sentence_labels'] = [np.nan] * len(sentences)\n",
    "\n",
    "# Update 'None' labels to np.nan in 'manual_sentence_labels'\n",
    "annotator1_results['manual_sentence_labels'] = annotator1_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [np.nan if v is None else v for v in x]\n",
    ")\n",
    "\n",
    "# Replace 'WHO 0', 'WHO 1', etc. with the corresponding integer number in 'manual_sentence_labels'\n",
    "annotator1_results['manual_sentence_labels'] = annotator1_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [int(v.split()[1]) if isinstance(v, str) and v.startswith(\"WHO\") else v for v in x]\n",
    ")\n",
    "\n",
    "# Copy 'manual_sentence_labels' to 'relevance_manual' and replace values\n",
    "annotator1_results['relevance_manual'] = annotator1_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [0 if isinstance(v, float) and np.isnan(v) else 1 for v in x]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "annotator1_results['note_PS_manual'] = np.nan\n",
    "\n",
    "# Iterate over the rows in annotator1_results\n",
    "for idx, row in annotator1_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    # Look up the rows in annotator1 with the same note_nr\n",
    "    matches = annotator1[annotator1['note_nr'] == note_nr]\n",
    "    # Check if any of these rows contain 'report_type' in the 'sentence_text' column\n",
    "    report_type_match = matches[matches['inception_sentence'].str.contains('report_type', na=False)]\n",
    "    if not report_type_match.empty:\n",
    "        # Assign the value from the 'WHOPS' column of the first matching row to 'note_PS_manual'\n",
    "        annotator1_results.at[idx, 'note_PS_manual'] = report_type_match['WHOPS'].values[0]\n",
    "\n",
    "        \n",
    "        \n",
    "# Convert 'WHO 0', 'WHO 1', etc. in 'note_PS_manual' to their corresponding integers and ensure NaN values are np.nan\n",
    "annotator1_results['note_PS_manual'] = annotator1_results['note_PS_manual'].apply(\n",
    "    lambda x: int(x.split()[1]) if isinstance(x, str) and x.startswith(\"WHO\") else np.nan\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copy the 'note_PS_manual' column and replace values\n",
    "annotator1_results['relevance_PS_manual'] = annotator1_results['note_PS_manual'].apply(\n",
    "    lambda x: 0 if np.isnan(x) else 1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the new column 'previous_ann' initialized with 'no'\n",
    "annotator1_results['previous_ann'] = 0\n",
    "\n",
    "# Iterate over the rows in annotator1_results\n",
    "for idx, row in annotator1_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    # Look up the rows in annotator1 with the same note_nr\n",
    "    matches = annotator1[annotator1['note_nr'] == note_nr]\n",
    "    # Check if any of these rows contain 'WHO PS already annotated' in the 'WHOPScat' column\n",
    "    if any(matches['WHOPScat'].str.contains('WHO PS already annotated', na=False)):\n",
    "        annotator1_results.at[idx, 'previous_ann'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels for annotator2\n",
    "\n",
    "# Assuming sample_selec and annotator2 are already defined DataFrames\n",
    "\n",
    "# Copy sample_selec to annotator2_results\n",
    "annotator2_results = copy.deepcopy(sample_selec)\n",
    "annotator2_note_nrs = list(sample_selec2['note_nr'])\n",
    "annotator2_results = annotator2_results[annotator2_results['note_nr'].isin(annotator2_note_nrs)]\n",
    "\n",
    "# Create a new column 'manual_sentence_labels' by copying the 'sentences' column\n",
    "annotator2_results['manual_sentence_labels'] = annotator2_results['sentences'].copy()\n",
    "\n",
    "# Iterate over the rows in annotator2_results\n",
    "for idx, row in annotator2_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    if note_nr in annotator2['note_nr'].values:\n",
    "        # Get the corresponding sentences for the note_nr\n",
    "        sentences = row['manual_sentence_labels']\n",
    "        updated_sentences = []\n",
    "#         for sentence in sentences:\n",
    "#             print('sentence:', sentence)\n",
    "#             match = annotator1[(annotator1['note_nr'] == note_nr) & (annotator1['_sentence_text'] == sentence)]\n",
    "#             if not match.empty:\n",
    "#                 updated_sentences.append(match['WHOPS'].values[0])\n",
    "#             else:\n",
    "#                 updated_sentences.append(np.nan)\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            # Initialize an empty list to hold the valid sentences\n",
    "            context = []\n",
    "            if context_flag:\n",
    "            # Check if sentences[i-1] exists\n",
    "                if i - 1 >= 0:\n",
    "                    context.append(sentences[i-1])\n",
    "            # Add sentences[i] (it is assumed that sentences[i] always exists)\n",
    "            context.append(sentences[i])\n",
    "            \n",
    "            if context_flag:\n",
    "            # Check if sentences[i+1] exists\n",
    "                if i + 1 < len(sentences):\n",
    "                    context.append(sentences[i+1])\n",
    "            # Join the valid sentences with spaces\n",
    "            sentence_context = '.'.join(context)\n",
    "            #print('sentence:', sentence)\n",
    "            #print('sentence_context', sentence_context)\n",
    "            # Find the match using the function\n",
    "            result = find_match(annotator1, note_nr, sentence, sentence_context, sentences, i)\n",
    "            sentences2.append(sentence_context)\n",
    "            # Append the corresponding value or np.nan to updated_sentences\n",
    "            if not result.empty:\n",
    "                if result['WHOPS'].values[0] == 'None':\n",
    "                    updated_sentences.append(np.nan)\n",
    "                else:\n",
    "                    updated_sentences.append(result['WHOPS'].values[0])\n",
    "            else:\n",
    "                updated_sentences.append(np.nan)\n",
    "        # Update the 'manual_sentence_labels' column with the new values\n",
    "        annotator2_results.at[idx,'sentences']=sentences2\n",
    "        annotator2_results.at[idx, 'manual_sentence_labels'] = updated_sentences\n",
    "\n",
    "# Iterate over the rows in annotator2_results again to update non-annotated sentences\n",
    "for idx, row in annotator2_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    sentences = row['manual_sentence_labels']\n",
    "    if all(isinstance(s, str) for s in sentences):\n",
    "        match = annotator2[annotator2['note_nr'] == note_nr]\n",
    "        if match.empty:\n",
    "            annotator2_results.at[idx, 'manual_sentence_labels'] = [np.nan] * len(sentences)\n",
    "\n",
    "# Update 'None' labels to np.nan in 'manual_sentence_labels'\n",
    "annotator2_results['manual_sentence_labels'] = annotator2_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [np.nan if v is None else v for v in x]\n",
    ")\n",
    "\n",
    "# Replace 'WHO 0', 'WHO 1', etc. with the corresponding integer number in 'manual_sentence_labels'\n",
    "annotator2_results['manual_sentence_labels'] = annotator2_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [int(v.split()[1]) if isinstance(v, str) and v.startswith(\"WHO\") else v for v in x]\n",
    ")\n",
    "\n",
    "# Copy 'manual_sentence_labels' to 'relevance_manual' and replace values\n",
    "annotator2_results['relevance_manual'] = annotator2_results['manual_sentence_labels'].apply(\n",
    "    lambda x: [0 if isinstance(v, float) and np.isnan(v) else 1 for v in x]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "annotator2_results['note_PS_manual'] = np.nan\n",
    "\n",
    "# Iterate over the rows in annotator2_results\n",
    "for idx, row in annotator2_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    # Look up the rows in annotator2 with the same note_nr\n",
    "    matches = annotator2[annotator2['note_nr'] == note_nr]\n",
    "    # Check if any of these rows contain 'report_type' in the 'sentence_text' column\n",
    "    report_type_match = matches[matches['inception_sentence'].str.contains('report_type', na=False)]\n",
    "    if not report_type_match.empty:\n",
    "        # Assign the value from the 'WHOPS' column of the first matching row to 'note_PS_manual'\n",
    "        annotator2_results.at[idx, 'note_PS_manual'] = report_type_match['WHOPS'].values[0]\n",
    "\n",
    "        \n",
    "        \n",
    "# Convert 'WHO 0', 'WHO 1', etc. in 'note_PS_manual' to their corresponding integers and ensure NaN values are np.nan\n",
    "annotator2_results['note_PS_manual'] = annotator2_results['note_PS_manual'].apply(\n",
    "    lambda x: int(x.split()[1]) if isinstance(x, str) and x.startswith(\"WHO\") else np.nan\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copy the 'note_PS_manual' column and replace values\n",
    "annotator2_results['relevance_PS_manual'] = annotator2_results['note_PS_manual'].apply(\n",
    "    lambda x: 0 if np.isnan(x) else 1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the new column 'previous_ann' initialized with 'no'\n",
    "annotator2_results['previous_ann'] = 0\n",
    "\n",
    "# Iterate over the rows in annotator2_results\n",
    "for idx, row in annotator2_results.iterrows():\n",
    "    note_nr = row['note_nr']\n",
    "    # Look up the rows in annotator1 with the same note_nr\n",
    "    matches = annotator2[annotator2['note_nr'] == note_nr]\n",
    "    # Check if any of these rows contain 'WHO PS already annotated' in the 'WHOPScat' column\n",
    "    if any(matches['WHOPScat'].str.contains('WHO PS already annotated', na=False)):\n",
    "        annotator2_results.at[idx, 'previous_ann'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f5b8b",
   "metadata": {},
   "source": [
    "# Select overlapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make backup of original\n",
    "annotator1_results_all = copy.deepcopy(annotator1_results)\n",
    "annotator2_results_all = copy.deepcopy(annotator2_results)\n",
    "sample_selec_all = copy.deepcopy(sample_selec)\n",
    "\n",
    "# create overlapping file datasets\n",
    "common_note_nrs = set(annotator1_results['note_nr']).intersection(annotator2_results['note_nr'])\n",
    "annotator1_results = annotator1_results[annotator1_results['note_nr'].isin(common_note_nrs)]\n",
    "annotator2_results = annotator2_results[annotator2_results['note_nr'].isin(common_note_nrs)]\n",
    "sample_selec = sample_selec[sample_selec['note_nr'].isin(common_note_nrs)]\n",
    "\n",
    "# delete any accidental double files\n",
    "annotator1_results = annotator1_results.drop_duplicates(subset=['note_nr', 'censored'], keep='first')\n",
    "annotator2_results = annotator2_results.drop_duplicates(subset=['note_nr', 'censored'], keep='first')\n",
    "\n",
    "\n",
    "annotator1_results = annotator1_results.reset_index(drop=True)\n",
    "annotator2_results = annotator2_results.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wether results annotators are aligned (brief overview)\n",
    "for col in ['manual_sentence_labels', 'relevance_manual', 'note_PS_manual', 'relevance_PS_manual', 'previous_ann']:\n",
    "    print('Col:', col)\n",
    "    for i in range(len(annotator1_results)):\n",
    "        if annotator1_results[col][i] != annotator2_results[col][i]:\n",
    "            print('note_nr:', annotator1_results['note_nr'][i])\n",
    "            print('ann1:', annotator1_results[col][i])\n",
    "            print('ann2:', annotator2_results[col][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dd42c",
   "metadata": {},
   "source": [
    "# Metrics for sentence level classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# Flatten the lists into one long list for each annotator\n",
    "labels_annotator1_flat = [item for sublist in annotator1_results['relevance_manual'] for item in sublist]\n",
    "labels_annotator2_flat = [item for sublist in annotator2_results['relevance_manual'] for item in sublist]\n",
    "\n",
    "# Convert to numerical values\n",
    "#labels_annotator1_num = [1 if x == 'relevant' else 0 for x in labels_annotator1_flat]\n",
    "#labels_annotator2_num = [1 if x == 'relevant' else 0 for x in labels_annotator2_flat]\n",
    "\n",
    "labels_annotator1_num = labels_annotator1_flat\n",
    "labels_annotator2_num = labels_annotator2_flat\n",
    "\n",
    "# Check the converted labels\n",
    "print(\"Labels Annotator 1 (numerical):\", labels_annotator1_num)\n",
    "print(\"Labels Annotator 2 (numerical):\", labels_annotator2_num)\n",
    "\n",
    "# Function to calculate precision, recall, and F1-score\n",
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Annotator 1 as ground truth\n",
    "precision_a1, recall_a1, f1_a1 = calculate_metrics(labels_annotator1_num, labels_annotator2_num)\n",
    "print(f\"Annotator 1 as ground truth -> Precision: {precision_a1}, Recall: {recall_a1}, F1-Score: {f1_a1}\")\n",
    "\n",
    "# Annotator 2 as ground truth\n",
    "precision_a2, recall_a2, f1_a2 = calculate_metrics(labels_annotator2_num, labels_annotator1_num)\n",
    "print(f\"Annotator 2 as ground truth -> Precision: {precision_a2}, Recall: {recall_a2}, F1-Score: {f1_a2}\")\n",
    "\n",
    "# Calculate Cohen's Kappa\n",
    "kappa = cohen_kappa_score(labels_annotator1_num, labels_annotator2_num)\n",
    "print(f\"Cohen's Kappa: {kappa}\")\n",
    "print()\n",
    "print(f\"Total sentences: {len(labels_annotator1_num)}\")\n",
    "print(f\"Total PS ann1: {labels_annotator1_num.count(1)}\")\n",
    "print(f\"Total NaN ann1: {labels_annotator1_num.count(0)}\")\n",
    "print(f\"Total PS ann2: {labels_annotator2_num.count(1)}\")\n",
    "print(f\"Total NaN ann2: {labels_annotator2_num.count(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bce9a4",
   "metadata": {},
   "source": [
    "# Metrics for sentence level regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa465b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import krippendorff\n",
    "\n",
    "# Sample data (you can replace this with your actual data)\n",
    "# annotator1_results = pd.DataFrame({'manual_sentence_labels': [[np.nan, 0, 1, np.nan, 2], [1, np.nan, 2, 3, 4]]})\n",
    "# annotator2_results = pd.DataFrame({'manual_sentence_labels': [[1, 0, np.nan, 2, 2], [np.nan, 1, 2, np.nan, 4]]})\n",
    "\n",
    "# Extract the columns\n",
    "#annotations1 = annotator1_results['manual_sentence_labels']\n",
    "#annotations2 = annotator2_results['manual_sentence_labels']\n",
    "\n",
    "#print('annotations1:', annotations1)\n",
    "#print('annotations2:', annotations2)\n",
    "\n",
    "# Flatten the lists into a single list and replace 'None' and any other non-numeric with np.nan\n",
    "flatten_annotations1 = [item for sublist in annotator1_results['manual_sentence_labels'] for item in sublist]\n",
    "flatten_annotations2 = [item for sublist in annotator2_results['manual_sentence_labels'] for item in sublist]\n",
    "\n",
    "\n",
    "# Convert to pandas Series\n",
    "annotations1_series = pd.Series(flatten_annotations1, dtype=float)\n",
    "annotations2_series = pd.Series(flatten_annotations2, dtype=float)\n",
    "\n",
    "#print('Flattened and cleaned annotations1:', annotations1_series)\n",
    "#print('Flattened and cleaned annotations2:', annotations2_series)\n",
    "\n",
    "# Print the number of NaNs in each series\n",
    "print('Number of NaNs in annotations1_series:', annotations1_series.isna().sum())\n",
    "print('Number of NaNs in annotations2_series:', annotations2_series.isna().sum())\n",
    "\n",
    "# Drop rows where either annotator has a NaN value for MAE calculation\n",
    "valid_indices_mae = annotations1_series.notna() & annotations2_series.notna()\n",
    "valid_annotations1_mae = annotations1_series[valid_indices_mae]\n",
    "valid_annotations2_mae = annotations2_series[valid_indices_mae]\n",
    "\n",
    "print('Valid annotations for MAE - Annotator 1:', valid_annotations1_mae)\n",
    "print('Valid annotations for MAE - Annotator 2:', valid_annotations2_mae)\n",
    "\n",
    "if len(valid_annotations1_mae) > 0 and len(valid_annotations2_mae) > 0:\n",
    "    # Calculate MAE assuming annotator 1 as the gold truth\n",
    "    mae_ann1_as_truth = mean_absolute_error(valid_annotations1_mae, valid_annotations2_mae)\n",
    "    \n",
    "    # Calculate MAE assuming annotator 2 as the gold truth\n",
    "    mae_ann2_as_truth = mean_absolute_error(valid_annotations2_mae, valid_annotations1_mae)\n",
    "    \n",
    "    # Print the MAE results\n",
    "    print(\"MAE (Annotator 1 as truth):\", mae_ann1_as_truth)\n",
    "    print(\"MAE (Annotator 2 as truth):\", mae_ann2_as_truth)\n",
    "else:\n",
    "    print(\"Insufficient valid data for MAE calculation.\")\n",
    "\n",
    "# Drop NaN values for Krippendorff's Alpha calculation\n",
    "valid_indices_ka = annotations1_series.notna() & annotations2_series.notna()\n",
    "valid_annotations1_ka = annotations1_series[valid_indices_ka]\n",
    "valid_annotations2_ka = annotations2_series[valid_indices_ka]\n",
    "\n",
    "print('Valid annotations for Krippendorff\\'s Alpha - Annotator 1:', valid_annotations1_ka)\n",
    "print('Valid annotations for Krippendorff\\'s Alpha - Annotator 2:', valid_annotations2_ka)\n",
    "\n",
    "# Check for sufficient variability and valid data\n",
    "if len(valid_annotations1_ka) > 0 and len(valid_annotations2_ka) > 0 and valid_annotations1_ka.nunique() > 1 and valid_annotations2_ka.nunique() > 1:\n",
    "    # Create an array for Krippendorff's Alpha\n",
    "    annotations_list = np.array([valid_annotations1_ka.values, valid_annotations2_ka.values])\n",
    "    \n",
    "    # Calculate Krippendorff's Alpha\n",
    "    alpha = krippendorff.alpha(reliability_data=annotations_list, level_of_measurement='nominal')\n",
    "    \n",
    "    # Print Krippendorff's Alpha\n",
    "    print(\"Krippendorff's Alpha:\", alpha)\n",
    "else:\n",
    "    print(\"Insufficient variability or valid data for Krippendorff's Alpha calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator1_results = annotator1_results.reset_index(drop=True)\n",
    "annotator2_results = annotator2_results.reset_index(drop=True)\n",
    "\n",
    "# check wether results annotators are aligned (brief overview)\n",
    "for col in ['manual_sentence_labels', 'relevance_manual', 'note_PS_manual', 'relevance_PS_manual', 'previous_ann']:\n",
    "    print('Col:', col)\n",
    "    for i in range(len(annotator1_results)):\n",
    "        print('Row:', i)\n",
    "        print('ann1:', annotator1_results[col][i])\n",
    "        print('ann2:', annotator2_results[col][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee7271",
   "metadata": {},
   "source": [
    "# Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double files docs\n",
    "annotator1_results['annotator'] = 'annotator1'\n",
    "annotator1_results['round'] = round_nr\n",
    "annotator2_results['annotator'] = 'annotator2'\n",
    "annotator2_results['round'] = round_nr\n",
    "\n",
    "# double files org\n",
    "df_iaa = pd.concat([annotator1_results, annotator2_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iaa.to_csv(f\"./Intermediate results/{round_nr}.csv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_NLP",
   "language": "python",
   "name": "llama_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
